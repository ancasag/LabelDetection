{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MxNetSSDExampleDD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R86up5pWLR3d"
      },
      "source": [
        "# Creating a MXNET model using SSD\n",
        "\n",
        "We first install the MXNET library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L9z9s3hGc4Rx",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4MPAM7JwKF3w",
        "colab": {}
      },
      "source": [
        "!pip install gluoncv\n",
        "!pip install mxnet\n",
        "!pip install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_I7JYjm_NhBk",
        "colab": {}
      },
      "source": [
        "!wget https://www.dropbox.com/s/2gr35m1alsghnzi/detecion.py?dl=1 -O detection.py\n",
        "!mv detection.py /usr/local/lib/python3.6/dist-packages/gluoncv/data/pascal_voc/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c37uXb1bLvIo",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vq2g0FM2gQWI"
      },
      "source": [
        "We download the dataset. If you are working with Google Colab, you have several options to download the dataset in this notebook, see the available options in the [LabelDetection documentation](https://github.com/ancasag/LabelDetection)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PR3qlP95mAZS",
        "colab": {}
      },
      "source": [
        "!unzip datasets.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "snKZVW8EPS1T"
      },
      "source": [
        "We import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FETPwOYXNKGR",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import mxnet as mx\n",
        "from mxnet import autograd, gluon\n",
        "import gluoncv as gcv\n",
        "from gluoncv.utils import download, viz\n",
        "from gluoncv.data import VOCDetection\n",
        "import argparse\n",
        "import importlib\n",
        "\n",
        "datasetName = \"dataset\"\n",
        "nepochs = 25\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DJTpqm8HbcP6",
        "colab": {}
      },
      "source": [
        "classes = ['apple','banana','orange']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l1IGSb86bf47",
        "colab": {}
      },
      "source": [
        "MXNET_ENABLE_GPU_P2P=0\n",
        "\n",
        "\n",
        "class VOCLike(VOCDetection):\n",
        "    CLASSES = classes\n",
        "    def __init__(self, root, splits, transform=None, index_map=None, preload_label=True):\n",
        "        super(VOCLike, self).__init__(root, splits, transform, index_map, preload_label)\n",
        "\n",
        "dataset = VOCLike(root='datasets', splits=((datasetName, 'train'),))\n",
        "test_dataset = VOCLike(root='datasets', splits=((datasetName, 'test'),))\n",
        "\n",
        "net = gcv.model_zoo.get_model('ssd_512_resnet50_v1_custom', classes=classes,\n",
        "    pretrained_base=False, transfer='voc')\n",
        "\n",
        "\n",
        "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
        "from gluoncv.data.transforms.presets.ssd import SSDDefaultTrainTransform,SSDDefaultValTransform\n",
        "\n",
        "def get_dataloader(net, train_dataset, val_dataset, data_shape, batch_size, num_workers):\n",
        "    \"\"\"Get dataloader.\"\"\"\n",
        "    width, height = data_shape, data_shape\n",
        "    # use fake data to generate fixed anchors for target generation\n",
        "    with autograd.train_mode():\n",
        "        _, _, anchors = net(mx.nd.zeros((1, 3, height, width)))\n",
        "    anchors = anchors.as_in_context(mx.cpu())\n",
        "    batchify_fn = Tuple(Stack(), Stack(), Stack())  # stack image, cls_targets, box_targets\n",
        "    train_loader = gluon.data.DataLoader(\n",
        "        train_dataset.transform(SSDDefaultTrainTransform(width, height, anchors)),\n",
        "        batch_size, True, batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n",
        "    val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
        "    val_loader = gluon.data.DataLoader(\n",
        "        val_dataset.transform(SSDDefaultValTransform(width, height)),\n",
        "        batch_size, False, batchify_fn=val_batchify_fn, last_batch='keep', num_workers=num_workers)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_data,test_data = get_dataloader(net, dataset,test_dataset, 512, 8, 0)\n",
        "\n",
        "\n",
        "# Start training(finetuning)\n",
        "n_gpu = mx.context.num_gpus()\n",
        "ctx = [mx.gpu(0)]\n",
        "net.collect_params().reset_ctx(ctx)\n",
        "trainer = gluon.Trainer(\n",
        "    net.collect_params(), 'sgd',\n",
        "    {'learning_rate': 0.001, 'wd': 0.0005, 'momentum': 0.9})\n",
        "\n",
        "mbox_loss = gcv.loss.SSDMultiBoxLoss()\n",
        "ce_metric = mx.metric.Loss('CrossEntropy')\n",
        "smoothl1_metric = mx.metric.Loss('SmoothL1')\n",
        "\n",
        "for epoch in range(0, nepochs):\n",
        "    ce_metric.reset()\n",
        "    smoothl1_metric.reset()\n",
        "    tic = time.time()\n",
        "    btic = time.time()\n",
        "    net.hybridize(static_alloc=True, static_shape=True)\n",
        "    for i, batch in enumerate(train_data):\n",
        "        batch_size = batch[0].shape[0]\n",
        "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
        "        cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
        "        box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
        "        with autograd.record():\n",
        "            cls_preds = []\n",
        "            box_preds = []\n",
        "            for x in data:\n",
        "                cls_pred, box_pred, _ = net(x)\n",
        "                cls_preds.append(cls_pred)\n",
        "                box_preds.append(box_pred)\n",
        "            sum_loss, cls_loss, box_loss = mbox_loss(\n",
        "                cls_preds, box_preds, cls_targets, box_targets)\n",
        "            autograd.backward(sum_loss)\n",
        "        # since we have already normalized the loss, we don't want to normalize\n",
        "        # by batch-size anymore\n",
        "        trainer.step(1)\n",
        "        ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
        "        smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
        "        name1, loss1 = ce_metric.get()\n",
        "        name2, loss2 = smoothl1_metric.get()\n",
        "        if i % 5 == 0:\n",
        "            print('[Epoch {}][Batch {}], Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}'.format(\n",
        "                epoch, i, batch_size/(time.time()-btic), name1, loss1, name2, loss2))\n",
        "        btic = time.time()\n",
        "    \n",
        "#############################################################################################\n",
        "# Save finetuned weights to disk\n",
        "net.save_parameters('ssd_512_resnet50_final.params')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QSMv7WmwwWqr"
      },
      "source": [
        "We evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rfct20DRnSEg",
        "colab": {}
      },
      "source": [
        "from gluoncv.utils.metrics.voc_detection import VOC07MApMetric\n",
        "\n",
        "def validate(net, val_data, ctx, eval_metric):\n",
        "    \"\"\"Test on validation dataset.\"\"\"\n",
        "    eval_metric.reset()\n",
        "    # set nms threshold and topk constraint\n",
        "    net.set_nms(nms_thresh=0.45, nms_topk=400)\n",
        "    net.hybridize(static_alloc=True, static_shape=True)\n",
        "    for batch in val_data:\n",
        "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
        "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
        "        det_bboxes = []\n",
        "        det_ids = []\n",
        "        det_scores = []\n",
        "        gt_bboxes = []\n",
        "        gt_ids = []\n",
        "        gt_difficults = []\n",
        "        for x, y in zip(data, label):\n",
        "            # get prediction results\n",
        "            ids, scores, bboxes = net(x)\n",
        "            det_ids.append(ids)\n",
        "            det_scores.append(scores)\n",
        "            # clip to image size\n",
        "            det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))\n",
        "            # split ground truths\n",
        "            gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n",
        "            gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n",
        "            gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n",
        "\n",
        "        # update metric\n",
        "        eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)\n",
        "    return eval_metric.get()\n",
        "\n",
        "\n",
        "val_metric = VOC07MApMetric(iou_thresh=0.5, class_names=test_dataset.classes)\n",
        "map_name, mean_ap = validate(net, test_data, ctx, val_metric)\n",
        "val_msg = '\\n'.join(['{}={}'.format(k, v) for k, v in zip(map_name, mean_ap)])\n",
        "print(val_msg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qMRa2ZhWQKXu"
      },
      "source": [
        "At the end you will have a file called ssd_512_resnet50_final.params that together with the names file can be included in the application to be employed with new images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MrIfv3N5Ob78"
      },
      "source": [
        "----------------------\n",
        "\n",
        "\n",
        "## Data distillation\n",
        "\n",
        "After training a model with the annotated images, it is possible to apply a data distillation procedure to create a model using the unlabelled images. You can only apply this techique if there were unlabelled images in your dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PL9uqjaMITw7"
      },
      "source": [
        "We first install the library for ensemble methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NiSCHcV0le_9",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/ancasag/ensembleObjectDetection.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMjNb4mKOg6e",
        "colab": {}
      },
      "source": [
        "cd ensembleObjectDetection/TestTimeAugmentation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N-BUUEloIXBP"
      },
      "source": [
        "We install additional libraries that are required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4CKivs9qOht3",
        "colab": {}
      },
      "source": [
        "!pip install clodsa\n",
        "!pip install mrcnn\n",
        "!pip install keras-retinanet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LdKAOsDKIcFm"
      },
      "source": [
        "The following cells apply data distillation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VAfN4Ng4Orpr",
        "colab": {}
      },
      "source": [
        "import testTimeAugmentation\n",
        "import function\n",
        "import os\n",
        "import shutil\n",
        "import argparse\n",
        "import ensembleOptions\n",
        "from mainTTA import tta\n",
        "from imutils import paths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E8pXdirAOssr",
        "colab": {}
      },
      "source": [
        "pathImg = '/content/datasets/unlabelled/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6D27rleOtq_",
        "colab": {}
      },
      "source": [
        "ssdResnet = testTimeAugmentation.MXnetSSD512Pred('/content/ssd_512_resnet50_final.params', '/content/datasets/classes.names')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7eKe_aGWPIHS",
        "colab": {}
      },
      "source": [
        "myTechniques = [ \"histo\",\"hflip\",\"none\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b548hFOfPJHQ",
        "colab": {}
      },
      "source": [
        "option = \"affirmative\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ET0IE5K_PJm3",
        "colab": {}
      },
      "source": [
        "tta(ssdResnet,myTechniques,pathImg,option)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j_6kvEq-2YVX"
      },
      "source": [
        "We remove the files without annotations. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TTumRwM62dZP",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from imutils import paths\n",
        "\n",
        "files = paths.list_files(\"/content/datasets/salida/output/\",validExts='.xml')\n",
        "i=0\n",
        "for fullpath in files:\n",
        "  if os.path.getsize(fullpath) < 400:   \n",
        "    name = fullpath[fullpath.rfind('/')+1:fullpath.rfind('.')]\n",
        "    os.remove(fullpath)\n",
        "    os.remove(\"/content/datasets/unlabelled/\"+name+\".jpg\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mvGlIbAqImU4"
      },
      "source": [
        "Now, we reorganize the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2dVZvL-mTj_S",
        "colab": {}
      },
      "source": [
        "!ls /content/datasets/unlabelled/*.jpg >> /content/datasets/VOCdataset/ImageSets/Main/train.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7PqF1LrxPS7M",
        "colab": {}
      },
      "source": [
        "!sed -i 's#/content/datasets/unlabelled/##g' /content/datasets/VOCdataset/ImageSets/Main/train.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nYWRCFYoPdpY",
        "colab": {}
      },
      "source": [
        "!sed -i 's#.jpg##g' /content/datasets/VOCdataset/ImageSets/Main/train.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jjr1emC0Pgnn",
        "colab": {}
      },
      "source": [
        "!mv /content/datasets/unlabelled/*.jpg /content/datasets/VOCdataset/JPEGImages/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8UDRH8P1PmIO",
        "colab": {}
      },
      "source": [
        "!mv /content/datasets/salida/output/*.xml /content/datasets/VOCdataset/Annotations/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TFUG-XDh4DrB",
        "colab": {}
      },
      "source": [
        "cd /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rGpGnmRZIpco"
      },
      "source": [
        "Finally, we retrain the model and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ziRmGJKnbo3r",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import mxnet as mx\n",
        "from mxnet import autograd, gluon\n",
        "import gluoncv as gcv\n",
        "from gluoncv.utils import download, viz\n",
        "from gluoncv.data import VOCDetection\n",
        "import argparse\n",
        "import importlib\n",
        "from gluoncv.utils.metrics.voc_detection import VOC07MApMetric\n",
        "\n",
        "datasetName = \"dataset\"\n",
        "nepochs = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uwBEhhS6bqn1",
        "colab": {}
      },
      "source": [
        "classes = ['apple','banana','orange']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vqFoK-RnQLPF",
        "colab": {}
      },
      "source": [
        "\n",
        "MXNET_ENABLE_GPU_P2P=0\n",
        "\n",
        "\n",
        "class VOCLike(VOCDetection):\n",
        "    CLASSES = classes\n",
        "    def __init__(self, root, splits, transform=None, index_map=None, preload_label=True):\n",
        "        super(VOCLike, self).__init__(root, splits, transform, index_map, preload_label)\n",
        "\n",
        "dataset = VOCLike(root='datasets', splits=((datasetName, 'train'),))\n",
        "test_dataset = VOCLike(root='datasets', splits=((datasetName, 'test'),))\n",
        "\n",
        "net = gcv.model_zoo.get_model('ssd_512_resnet50_v1_custom', classes=classes,\n",
        "    pretrained_base=False, transfer='voc')\n",
        "net.load_params('ssd_512_resnet50_final.params')\n",
        "\n",
        "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
        "from gluoncv.data.transforms.presets.ssd import SSDDefaultTrainTransform,SSDDefaultValTransform\n",
        "\n",
        "def get_dataloader(net, train_dataset, val_dataset, data_shape, batch_size, num_workers):\n",
        "    \"\"\"Get dataloader.\"\"\"\n",
        "    width, height = data_shape, data_shape\n",
        "    # use fake data to generate fixed anchors for target generation\n",
        "    with autograd.train_mode():\n",
        "        _, _, anchors = net(mx.nd.zeros((1, 3, height, width)))\n",
        "    anchors = anchors.as_in_context(mx.cpu())\n",
        "    batchify_fn = Tuple(Stack(), Stack(), Stack())  # stack image, cls_targets, box_targets\n",
        "    train_loader = gluon.data.DataLoader(\n",
        "        train_dataset.transform(SSDDefaultTrainTransform(width, height, anchors)),\n",
        "        batch_size, True, batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n",
        "    val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
        "    val_loader = gluon.data.DataLoader(\n",
        "        val_dataset.transform(SSDDefaultValTransform(width, height)),\n",
        "        batch_size, False, batchify_fn=val_batchify_fn, last_batch='keep', num_workers=num_workers)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_data,test_data = get_dataloader(net, dataset,test_dataset, 512, 8, 0)\n",
        "\n",
        "\n",
        "# Start training(finetuning)\n",
        "n_gpu = mx.context.num_gpus()\n",
        "ctx = [mx.gpu(0)]\n",
        "net.collect_params().reset_ctx(ctx)\n",
        "trainer = gluon.Trainer(\n",
        "    net.collect_params(), 'sgd',\n",
        "    {'learning_rate': 0.001, 'wd': 0.0005, 'momentum': 0.9})\n",
        "\n",
        "mbox_loss = gcv.loss.SSDMultiBoxLoss()\n",
        "ce_metric = mx.metric.Loss('CrossEntropy')\n",
        "smoothl1_metric = mx.metric.Loss('SmoothL1')\n",
        "\n",
        "for epoch in range(0, nepochs):\n",
        "    ce_metric.reset()\n",
        "    smoothl1_metric.reset()\n",
        "    tic = time.time()\n",
        "    btic = time.time()\n",
        "    net.hybridize(static_alloc=True, static_shape=True)\n",
        "    for i, batch in enumerate(train_data):\n",
        "        batch_size = batch[0].shape[0]\n",
        "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
        "        cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
        "        box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
        "        with autograd.record():\n",
        "            cls_preds = []\n",
        "            box_preds = []\n",
        "            for x in data:\n",
        "                cls_pred, box_pred, _ = net(x)\n",
        "                cls_preds.append(cls_pred)\n",
        "                box_preds.append(box_pred)\n",
        "            sum_loss, cls_loss, box_loss = mbox_loss(\n",
        "                cls_preds, box_preds, cls_targets, box_targets)\n",
        "            autograd.backward(sum_loss)\n",
        "        # since we have already normalized the loss, we don't want to normalize\n",
        "        # by batch-size anymore\n",
        "        trainer.step(1)\n",
        "        ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
        "        smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
        "        name1, loss1 = ce_metric.get()\n",
        "        name2, loss2 = smoothl1_metric.get()\n",
        "        if i % 5 == 0:\n",
        "            print('[Epoch {}][Batch {}], Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}'.format(\n",
        "                epoch, i, batch_size/(time.time()-btic), name1, loss1, name2, loss2))\n",
        "        btic = time.time()\n",
        "    \n",
        "#############################################################################################\n",
        "# Save finetuned weights to disk\n",
        "net.save_parameters('ssd_512_resnet50_finalDD.params')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OJL308A5Qdqw",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def validate(net, val_data, ctx, eval_metric):\n",
        "    \"\"\"Test on validation dataset.\"\"\"\n",
        "    eval_metric.reset()\n",
        "    # set nms threshold and topk constraint\n",
        "    net.set_nms(nms_thresh=0.45, nms_topk=400)\n",
        "    net.hybridize(static_alloc=True, static_shape=True)\n",
        "    for batch in val_data:\n",
        "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
        "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
        "        det_bboxes = []\n",
        "        det_ids = []\n",
        "        det_scores = []\n",
        "        gt_bboxes = []\n",
        "        gt_ids = []\n",
        "        gt_difficults = []\n",
        "        for x, y in zip(data, label):\n",
        "            # get prediction results\n",
        "            ids, scores, bboxes = net(x)\n",
        "            det_ids.append(ids)\n",
        "            det_scores.append(scores)\n",
        "            # clip to image size\n",
        "            det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))\n",
        "            # split ground truths\n",
        "            gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n",
        "            gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n",
        "            gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n",
        "\n",
        "        # update metric\n",
        "        eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)\n",
        "    return eval_metric.get()\n",
        "\n",
        "\n",
        "val_metric = VOC07MApMetric(iou_thresh=0.5, class_names=test_dataset.classes)\n",
        "map_name, mean_ap = validate(net, test_data, ctx, val_metric)\n",
        "val_msg = '\\n'.join(['{}={}'.format(k, v) for k, v in zip(map_name, mean_ap)])\n",
        "print(val_msg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t9zPtr5cQlek",
        "colab": {}
      },
      "source": [
        "-------------------------------\n",
        "\n",
        "# Using the model in LabelDetection\n",
        "\n",
        "If you want to use the trained model with LabelDetection, you must download the following files:\n",
        "- ssd_512_resnet50_finalDD.params\n",
        "- datasets/classes.names"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}